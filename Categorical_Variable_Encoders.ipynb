{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import big_o\n",
    "import missingno as mso\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from category_encoders import BinaryEncoder, HelmertEncoder, WOEEncoder, LeaveOneOutEncoder, JamesSteinEncoder, MEstimateEncoder\n",
    "from sklearn.feature_extraction import FeatureHasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.0\n",
      "Gd\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('E:\\\\product\\\\Test_Data\\\\supervised Data\\\\train.csv')\n",
    "start_time_ = time.time()\n",
    "# identify columns which have missing values\n",
    "def columns_with_missing(data):\n",
    "        '''\n",
    "        funcion that search for the columns with missing data also find the \n",
    "        percentage of missing data in each columns and build and return a\n",
    "        dataframe with two columns one is features with missing data and\n",
    "        other column  have percentage of values missing in that feature\n",
    "        args:\n",
    "            data -> our unprocessed dataframe\n",
    "\n",
    "        '''\n",
    "        # percentage of missing values in each column\n",
    "        percentage_miss=((data.isnull().sum()/data.isnull().count())*100).sort_values(ascending=False)\n",
    "        # build a dataframe    \n",
    "        missing_df = pd.concat([percentage_miss], axis=1, keys=['%missing'])\n",
    "        missing_df.insert(0, 'feature', missing_df.index)\n",
    "        # conditions for filtering \n",
    "        cond1 = missing_df['%missing']>0 \n",
    "        cond2 = missing_df['%missing']<50\n",
    "        cond3 = missing_df['%missing']<=10\n",
    "        cond4 = missing_df['%missing']>10\n",
    "        final_cond_1= cond1 & cond2 & cond4\n",
    "        final_cond_2= cond1 & cond3\n",
    "        # list of columns with 50% or more missing values\n",
    "        col_greater_than_50per = missing_df.loc[missing_df['%missing'] >50]['feature'].tolist()\n",
    "        # list of columns with less than 10% missing values\n",
    "        col_less_than_10per = missing_df.loc[final_cond_2]['feature'].tolist()\n",
    "        # remaning columns\n",
    "        remaining_col = missing_df.loc[final_cond_1]\n",
    "        # reset the index of dataframe \n",
    "        remaining_col.reset_index(level=0, drop=True, inplace=True)\n",
    "        # convert remaining_col into a dictionary because we have only two columns left so ease for searching\n",
    "        remaining_co_l = remaining_col['feature'].tolist()\n",
    "        return remaining_co_l, col_greater_than_50per, col_less_than_10per\n",
    "\n",
    "# perform operations on column\n",
    "def operations(col_greater50, col, col_less10, data):\n",
    "    \n",
    "    '''\n",
    "    Imputation Techniques\n",
    "    '''\n",
    "    # mean imputation\n",
    "    def mean_imputation(data, list_of_col):\n",
    "        for col in list_of_col:\n",
    "            data[col] = data[col].fillna(value=data[col].mean())\n",
    "        return data\n",
    "    # mode imputation\n",
    "    def median_imputation(data, list_of_col):\n",
    "        for col in list_of_col:\n",
    "            data[col] = data[col].fillna(value=data[col].median())\n",
    "        return data\n",
    "    # median imputation\n",
    "    def mode_imputation(data, list_of_col):\n",
    "        for col in list_of_col:\n",
    "            data[col] = data[col].fillna(value=data[col].mode()[0])\n",
    "        return data\n",
    "    '''\n",
    "    Deletion techniques\n",
    "    '''\n",
    "    # columns with more than 50% values are missing must be dropped\n",
    "    col_greater50 = [col for col in col_greater50 if data[col].dtypes != np.object]\n",
    "    data.drop(col_greater50, axis=1, inplace=True)\n",
    "    # delete the rows with missing data\n",
    "    col_less10 = [col for col in col_less10 if data[col].dtypes != np.object]\n",
    "    data.dropna(how='any', subset=col_less10, inplace=True)\n",
    "\n",
    "# run both the function \n",
    "colums, colums50, columns10 = columns_with_missing(df)\n",
    "operations(col_greater50=colums50, col=colums, col_less10=columns10, data=df)\n",
    "# mean imputation\n",
    "df['LotFrontage'] = df['LotFrontage'].fillna(value=df['LotFrontage'].median())\n",
    "print(df['LotFrontage'].median())\n",
    "df['FireplaceQu'] = df['FireplaceQu'].fillna(value=df['FireplaceQu'].mode()[0])\n",
    "print(df['FireplaceQu'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CATEGORICAL VARAIBLE  ENCODING TECHNIQUES\n",
    "   ## 1.OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sklearn OneHotEncoder\n",
    "def sk_onehotencoder(data):\n",
    "    # Identify the categorical features\n",
    "    features = [x for x in data.columns if x not in ['Id', 'SalePrice'] and data[x].dtypes == np.object]\n",
    "        \n",
    "    # fill the NaN values with NONE\n",
    "    for col in features:\n",
    "        data.loc[:, col] = data.fillna('NONE').astype(str)\n",
    "\n",
    "    # one hot encoder\n",
    "    ohe = OneHotEncoder()\n",
    "    # fit on data\n",
    "    ohe.fit(data[features])\n",
    "    # new_data\n",
    "    train_data = ohe.transform(data[features])\n",
    "    return train_data\n",
    "# a sparse matrix\n",
    "_df = sk_onehotencoder(data=df)\n",
    "\n",
    "# using get_dummies function\n",
    "def pd_getdummies(data):\n",
    "    # using pandas get_dummies function\n",
    "    _data = pd.get_dummies(data)\n",
    "    return _data\n",
    "# a dataframe\n",
    "df_GD = pd_getdummies(data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Dummy Variable encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also using get_dummies but slightest change\n",
    "def dummyVarencode(data):\n",
    "    # using get_dummies\n",
    "    _data = pd.get_dummies(data, drop_first=True)\n",
    "    return _data\n",
    "# our dataframe is\n",
    "df_DVE = dummyVarencode(data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Binary encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csvya\\anaconda3\\lib\\site-packages\\category_encoders\\utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "# number of class k basis p lagani hai\n",
    "def binary_encoder(data):\n",
    "    # extract all features which are object\n",
    "    categorical_features = [col for col in data.columns if data[col].dtypes == np.object]\n",
    "    # using binary encoder\n",
    "    BE = BinaryEncoder(cols=categorical_features)\n",
    "    _data = BE.fit_transform(df[categorical_features])\n",
    "    return _data\n",
    "# let's see\n",
    "df_BE = binary_encoder(data=df)\n",
    "# make a copy of df\n",
    "df_BE_2 = df.copy()\n",
    "#  drop all the categorical column first from orignal dataframe\n",
    "df_BE_2.drop(columns=[col for col in df.columns if df[col].dtypes == np.object], inplace=True)\n",
    "#  final dataframe\n",
    "df_BE_Final = pd.concat([df_BE_2, df_BE], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Frequency encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "def frequency_encoder(data):\n",
    "    # create a copy of data\n",
    "    _data = data.copy()\n",
    "    # find the categorical features\n",
    "    features = [col for col in data.columns if data[col].dtypes == np.object]\n",
    "    print(len(features))\n",
    "    # iterate over each col in list of\n",
    "    # features and apply frequency encodin\n",
    "    # to each column and map these values\n",
    "    # in orignal column\n",
    "    for feat in features:\n",
    "        # find the probability with respect to total values\n",
    "        freq = _data.groupby(feat).size()/len(_data)\n",
    "        # map this values in the orignal column\n",
    "        _data[feat] = _data[feat].map(freq)\n",
    "    # return the data\n",
    "    return _data\n",
    "# let's see\n",
    "df_FE = frequency_encoder(data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Helmert Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csvya\\anaconda3\\lib\\site-packages\\category_encoders\\utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "features = [col for col in df.columns if df[col].dtypes == np.object]\n",
    "def helmert_encoder(data,features):\n",
    "    _data = data.copy()\n",
    "    # create an instance of helmert encoder\n",
    "    encoder = HelmertEncoder(cols=features, drop_invariant=True)\n",
    "    dfh = encoder.fit_transform(df[features])\n",
    "    _data.drop(columns=features, axis=1, inplace=True)\n",
    "    _data = pd.concat([_data,dfh], axis=1)\n",
    "    return _data\n",
    "# let's see\n",
    "df_HE = helmert_encoder(data=df, features=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encoder(data, features, target):\n",
    "    # build copy of a dataset\n",
    "    _data = data.copy()\n",
    "    # find mean of each column with respect\n",
    "    # to target variable and map them in \n",
    "    # orignal column\n",
    "    for col in features:\n",
    "        # mean wrt target\n",
    "        mean_encode = _data.groupby(col)[target].mean()\n",
    "        # map in column\n",
    "        _data[col] = _data[col].map(mean_encode)\n",
    "    return _data\n",
    "df_TE = target_encoder(data=df, features=features, target = 'SalePrice')\n",
    "\n",
    "# target encoding using smoothing\n",
    "def smoothing(data, features, target):\n",
    "    # build copy of data\n",
    "    _data = data.copy()\n",
    "    # compute the global mean\n",
    "    mean = _data[target].mean()\n",
    "    for col in features:\n",
    "        # compute the number of values and mean of each group\n",
    "        agg = _data.groupby(col)[target].agg(['count', 'mean'])\n",
    "        counts = agg['count']\n",
    "        means = agg['mean']\n",
    "        weight = 100\n",
    "        # compute the 'smoothed' mean\n",
    "        smooth = (counts * means + weight * mean)/(counts + weight)\n",
    "        # replace each value by it's sooth mean\n",
    "        _data[col] = _data[col].map(smooth)\n",
    "    return _data\n",
    "df_SE = smoothing(data=df, features=features, target='SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.Weight of evidence encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ONLY WORK ON CALSSIFICATION PROBLEM MAINLY WHEN TARGET HAS BINARY CLASS\n",
    "'''\n",
    "def WOF_encoder(data, features, target):\n",
    "    # build copy of data\n",
    "    _data = data.copy()\n",
    "    # create an instance of WOEEncoder\n",
    "    WOE = WOEEncoder(cols=features, regularization=0.5, random_state=100)\n",
    "    _data[features] = WOE.fit_transform(_data[features], _data[target])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.Leave One Out encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csvya\\anaconda3\\lib\\site-packages\\category_encoders\\utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "def Leave_one_Out_Encoder(data, features, target):\n",
    "    # build copy of data\n",
    "    _data = data.copy()\n",
    "    # create an instance of leave one out encoder\n",
    "    LOOE = LeaveOneOutEncoder(cols=features, sigma=0.2)\n",
    "    _data[features] = LOOE.fit_transform(_data[features], _data[target])\n",
    "    return _data\n",
    "df_LOOE = Leave_one_Out_Encoder(data=df, features=features, target='SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.James-Stein encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csvya\\anaconda3\\lib\\site-packages\\category_encoders\\utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "def james_stein_encoder(data, features, target):\n",
    "    # build copy of data\n",
    "    _data = data.copy()\n",
    "    # create an instance of james-stein encoder\n",
    "    js_encode = JamesSteinEncoder(cols=features, drop_invariant=True, sigma= 0.2, random_state=10)\n",
    "    _data[features] = js_encode.fit_transform(_data[features], _data[target])\n",
    "    return _data\n",
    "df_JSE = james_stein_encoder(data=df, features=features, target='SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.M Estimator Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csvya\\anaconda3\\lib\\site-packages\\category_encoders\\utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "def m_estimator_encoder(data, features, target):\n",
    "    # build copy of data\n",
    "    _data = data.copy()\n",
    "    # create an instance of M estimator encoder\n",
    "    mee = MEstimateEncoder(cols=features, m=2, drop_invariant=True)\n",
    "    _data[features] = mee.fit_transform(_data[features], _data[target])\n",
    "    return _data\n",
    "df_MEE = m_estimator_encoder(data=df, features=features, target='SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "many more techniques will come soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
